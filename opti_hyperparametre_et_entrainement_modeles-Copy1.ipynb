{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7840510d-a150-4926-9e9c-4a681ec765aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import mlflow\n",
    "import time\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd0c91-2ebd-4698-8fcf-89f67f2e292b",
   "metadata": {},
   "source": [
    "## Séparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63055f91-6b57-480e-804c-f0c5ca9b452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"utils_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c216b2c-811e-4e6f-aa29-b05f2f95c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "new_df = new_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f863cc5f-357e-40fe-9c7d-f9cc45ecd7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.set_index('SK_ID_CURR', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0eb1973-b93b-433f-9e3e-cd6d20a117a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.drop(columns='TARGET')\n",
    "y = new_df['TARGET']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = pd.DataFrame(scaler.transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ca7df9-c2b1-420d-a398-f33aecb39a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d46527-da6c-4c8c-ae7b-1c2b7282fc48",
   "metadata": {},
   "source": [
    "## Équilibrage des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "866412a1-75d3-4bee-b05b-ddebe3c4fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "X_train_over, y_train_over = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922a9d05-fa50-4fe2-b276-6c7f34b6b5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 197863, 1: 17391}) Counter({0: 197863, 1: 197863})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counter_before = Counter(y_train)\n",
    "counter_after = Counter(y_train_over)\n",
    "print(counter_before, counter_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065a4a9a-6d00-4bc2-a07c-e623cdb4df6f",
   "metadata": {},
   "source": [
    "## Entrainement des modèles\n",
    "### Réduction des données\n",
    "En l'état les données sont trop volumineuses pour pouvoir entrainer les modèles. Nous pouvons les réduire en utilisant la méthode sample() d'un dataframe pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9211fcb2-7dbe-4fa8-a160-8dc23e3ec532",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sample = X_train_over.merge(y_train_over, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bdc77b2-41ae-42a6-ab40-e7546d06f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_sample = to_sample.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "afda98fc-8194-4e81-92c6-c35e943512da",
   "metadata": {},
   "source": [
    "X_train_sample = to_sample.drop(columns='TARGET')\n",
    "y_train_sample = to_sample['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "051e68ea-597f-45ec-8df1-d6bce2f373a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = to_sample.drop(columns='TARGET')\n",
    "y_train = to_sample['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce54a9c-0693-4f7e-900a-fb7aa6b3b05a",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40d692a5-e222-4dc1-8360-597d0246e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a602f124-11f0-4379-94cc-29384410789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "kfolds = KFold(n_splits=5, shuffle=True, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "184f6583-a5c7-4dee-a204-ffa5b08dcad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tune(objective, study_name):\n",
    "    study = optuna.create_study(study_name=study_name, direction='maximize')\n",
    "    \n",
    "    study.optimize(objective, n_trials=10)\n",
    "    \n",
    "    params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    return params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "277882ff-a2e3-40df-93bc-23c3b23192da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_objective(trial):\n",
    "    _C = trial.suggest_float('C', 0.01, 100)\n",
    "    \n",
    "    clf_log = LogisticRegression(C=_C, random_state=random_seed)\n",
    "    \n",
    "    score = cross_val_score(clf_log, X_train, y_train, cv=kfolds, scoring='roc_auc').mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e8cb5d1-8f44-4389-84a3-eddf49f1bcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/matth/Documents/openclassroom/data_scientist/P7/mlruns/747928904526590890', creation_time=1672240642707, experiment_id='747928904526590890', last_update_time=1672240642707, lifecycle_stage='active', name='optimisation hyperparamètre et entrainement des modèles', tags={}>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('optimisation hyperparamètre et entrainement des modèles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad186719-e3d2-4fdf-81af-4314e1a900e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-11 11:56:09,379]\u001b[0m A new study created in memory with name: log_reg\u001b[0m\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-01-11 11:56:13,695]\u001b[0m Trial 0 finished with value: 0.7915643731331709 and parameters: {'C': 58.3391622334778}. Best is trial 0 with value: 0.7915643731331709.\u001b[0m\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-01-11 11:56:18,524]\u001b[0m Trial 1 finished with value: 0.7915590182096277 and parameters: {'C': 70.49290393132921}. Best is trial 0 with value: 0.7915643731331709.\u001b[0m\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-01-11 11:56:22,951]\u001b[0m Trial 2 finished with value: 0.7915633131925986 and parameters: {'C': 82.55471974284607}. Best is trial 0 with value: 0.7915643731331709.\u001b[0m\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-01-11 11:56:27,329]\u001b[0m Trial 3 finished with value: 0.7915606056009322 and parameters: {'C': 78.6834513861257}. Best is trial 0 with value: 0.7915643731331709.\u001b[0m\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-01-11 11:56:31,722]\u001b[0m Trial 4 finished with value: 0.7915612055191429 and parameters: {'C': 3.1972103640035523}. Best is trial 0 with value: 0.7915643731331709.\u001b[0m\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-01-11 11:56:36,096]\u001b[0m Trial 5 finished with value: 0.7915640344038855 and parameters: {'C': 23.42661858359282}. Best is trial 0 with value: 0.7915643731331709.\u001b[0m\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-01-11 11:56:40,654]\u001b[0m Trial 6 finished with value: 0.7915612853224723 and parameters: {'C': 17.30784886841978}. Best is trial 0 with value: 0.7915643731331709.\u001b[0m\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-01-11 11:56:45,145]\u001b[0m Trial 7 finished with value: 0.7915600847529076 and parameters: {'C': 30.84003425694298}. Best is trial 0 with value: 0.7915643731331709.\u001b[0m\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "\u001b[32m[I 2023-01-11 11:56:49,633]\u001b[0m Trial 8 finished with value: 0.7915616428595056 and parameters: {'C': 58.117936011662756}. Best is trial 0 with value: 0.7915643731331709.\u001b[0m\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"Logistic regression\"):\n",
    "    params = {}\n",
    "    params['MODEL_NAME'] = 'logistic_regression'\n",
    "    params['TRAIN_SIZE'] = len(X_train)\n",
    "    \n",
    "    start = time.time()\n",
    "    log_params, log_score = tune(log_reg_objective, 'log_reg')\n",
    "    params['time'] = time.time() - start\n",
    "    \n",
    "    params['C'] = log_params['C']\n",
    "    params['train_roc_auc'] = log_score\n",
    "    train_roc_auc = log_score\n",
    "    mlflow.log_metric('train_roc_auc', train_roc_auc) \n",
    "    train_roc_auc = log_score\n",
    "    \n",
    "    # On entraine le modèle avec les paramètres retournés\n",
    "    clf_log = LogisticRegression(C=params['C'])\n",
    "    clf_log.fit(X_train, y_train)\n",
    "    \n",
    "    # On calcul le score final du modele avec les données de test\n",
    "    y_pred = clf_log.predict(X_test)\n",
    "    params['test_roc_auc'] = roc_auc_score(y_test, y_pred)\n",
    "    mlflow.log_metric('test_roc_auc',roc_auc_score(y_test, y_pred))\n",
    "    \n",
    "    # calcul de la matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='d')\n",
    "    plt.savefig(\"confusion_matrix_hors_metier.png\")\n",
    "\n",
    "    mlflow.log_artifact(local_path=\"confusion_matrix_hors_metier.png\")\n",
    "    mlflow.log_params(params)\n",
    "    # On sauvegarde le model pour l'utiliser dans le futur sans avoir à l'entrainer de nouveau\n",
    "    mlflow.sklearn.log_model(clf_log, 'log_reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aae0d0-25f3-43a4-bfa9-24b1b18cba5d",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63e6361a-3491-436a-8ad8-4ea3a9b7dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def randomforest_objective(trial):\n",
    "    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n",
    "    _max_depth = trial.suggest_int(\"max_depth\", 5, 10)\n",
    "    \n",
    "    \n",
    "    clf_forest = RandomForestClassifier(max_depth=_max_depth,\n",
    "                                        n_estimators=_n_estimators,\n",
    "                                        random_state=random_seed)\n",
    "    \n",
    "    score = cross_val_score(clf_forest,\n",
    "                            X_train,\n",
    "                            y_train,\n",
    "                            cv=kfolds,\n",
    "                            scoring='roc_auc').mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edc08106-0a55-45e1-b4fa-9209727fc8a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-28 16:18:24,948]\u001b[0m A new study created in memory with name: log_reg\u001b[0m\n",
      "\u001b[32m[I 2022-12-28 16:19:29,564]\u001b[0m Trial 0 finished with value: 0.9119124131897799 and parameters: {'n_estimators': 61, 'max_depth': 6}. Best is trial 0 with value: 0.9119124131897799.\u001b[0m\n",
      "\u001b[32m[I 2022-12-28 16:21:18,416]\u001b[0m Trial 1 finished with value: 0.9464632727402644 and parameters: {'n_estimators': 65, 'max_depth': 10}. Best is trial 1 with value: 0.9464632727402644.\u001b[0m\n",
      "\u001b[32m[I 2022-12-28 16:23:04,974]\u001b[0m Trial 2 finished with value: 0.9387903948551776 and parameters: {'n_estimators': 70, 'max_depth': 9}. Best is trial 1 with value: 0.9464632727402644.\u001b[0m\n",
      "\u001b[32m[I 2022-12-28 16:25:14,302]\u001b[0m Trial 3 finished with value: 0.9316418840029149 and parameters: {'n_estimators': 94, 'max_depth': 8}. Best is trial 1 with value: 0.9464632727402644.\u001b[0m\n",
      "\u001b[32m[I 2022-12-28 16:30:01,457]\u001b[0m Trial 4 finished with value: 0.9471277083622139 and parameters: {'n_estimators': 185, 'max_depth': 10}. Best is trial 4 with value: 0.9471277083622139.\u001b[0m\n",
      "\u001b[32m[I 2022-12-28 16:31:46,167]\u001b[0m Trial 5 finished with value: 0.9464580456152575 and parameters: {'n_estimators': 68, 'max_depth': 10}. Best is trial 4 with value: 0.9471277083622139.\u001b[0m\n",
      "\u001b[32m[I 2022-12-28 16:33:04,228]\u001b[0m Trial 6 finished with value: 0.9371309746671453 and parameters: {'n_estimators': 52, 'max_depth': 9}. Best is trial 4 with value: 0.9471277083622139.\u001b[0m\n",
      "\u001b[32m[I 2022-12-28 16:36:54,545]\u001b[0m Trial 7 finished with value: 0.932177275785137 and parameters: {'n_estimators': 179, 'max_depth': 8}. Best is trial 4 with value: 0.9471277083622139.\u001b[0m\n",
      "\u001b[32m[I 2022-12-28 16:38:55,162]\u001b[0m Trial 8 finished with value: 0.9216641111794267 and parameters: {'n_estimators': 108, 'max_depth': 7}. Best is trial 4 with value: 0.9471277083622139.\u001b[0m\n",
      "\u001b[32m[I 2022-12-28 16:40:56,230]\u001b[0m Trial 9 finished with value: 0.9216641111794267 and parameters: {'n_estimators': 108, 'max_depth': 7}. Best is trial 4 with value: 0.9471277083622139.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"Random Forest\"):\n",
    "    params = {}\n",
    "    params['MODEL_NAME'] = 'Random Forest'\n",
    "    params['TRAIN_SIZE'] = len(X_train)\n",
    "    \n",
    "    # Optimisation des hyperparamètres\n",
    "    start = time.time()\n",
    "    forest_params, forest_score = tune(randomforest_objective)\n",
    "    params['time'] = time.time() - start\n",
    "    \n",
    "    params['n_estimators'] = forest_params['n_estimators']\n",
    "    params['max_depth'] = forest_params['max_depth']\n",
    "    train_roc_auc = forest_score\n",
    "    mlflow.log_metric('train_roc_auc', train_roc_auc)\n",
    "    \n",
    "    # On entraine le modèle avec les paramètres retournés\n",
    "    clf_forest = RandomForestClassifier(n_estimators=params['n_estimators'],\n",
    "                                        max_depth=params['max_depth'],\n",
    "                                        random_state=random_seed)\n",
    "    clf_forest.fit(X_train, y_train)\n",
    "    \n",
    "    # On calcule le score final du modele avec les données test\n",
    "    y_pred = clf_forest.predict(X_test)\n",
    "    mlflow.log_metric('test_roc_auc',roc_auc_score(y_test, y_pred))\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # On sauvegarde le modele\n",
    "    mlflow.sklearn.log_model(clf_forest, 'Random forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cfa5bc-634c-4f02-b754-3bf1e67b255b",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97e353b1-89ff-4409-8c03-17cefd7d4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def xgboost_objective(trial):\n",
    "    _n_estimators = trial.suggest_int(\"n_estimator\", 10, 100, 10)\n",
    "    _max_depth = trial.suggest_int('max_depth', 5, 25, 2)\n",
    "    #_learning_rate = trial.suggest_float('learning_rate', 0.01, 0.1, step=0.01)\n",
    "    \n",
    "    \n",
    "    clf_boost = GradientBoostingClassifier(max_depth=_max_depth,\n",
    "                                           n_estimators=_n_estimators,\n",
    "                                           #learning_rate=_learning_rate,\n",
    "                                           random_state=random_seed)\n",
    "    \n",
    "    score = cross_val_score(clf_boost, X_train, y_train, cv=kfolds, scoring='roc_auc').mean()\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a2216-dd33-4d12-98ec-565ae8479f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('optimisation cout metier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7ff1c-f30a-4f77-a049-5d8e4394e53f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-04 12:03:18,623]\u001b[0m A new study created in memory with name: xgboost\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 12:58:48,522]\u001b[0m Trial 0 finished with value: 0.9780936735321244 and parameters: {'n_estimator': 100, 'max_depth': 11}. Best is trial 0 with value: 0.9780936735321244.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 13:43:31,515]\u001b[0m Trial 1 finished with value: 0.9783889462472363 and parameters: {'n_estimator': 70, 'max_depth': 13}. Best is trial 1 with value: 0.9783889462472363.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 13:45:59,678]\u001b[0m Trial 2 finished with value: 0.9342093088373022 and parameters: {'n_estimator': 10, 'max_depth': 5}. Best is trial 1 with value: 0.9783889462472363.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 13:53:09,011]\u001b[0m Trial 3 finished with value: 0.9631364863661851 and parameters: {'n_estimator': 30, 'max_depth': 5}. Best is trial 1 with value: 0.9783889462472363.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 14:32:50,833]\u001b[0m Trial 4 finished with value: 0.9774930891104046 and parameters: {'n_estimator': 70, 'max_depth': 11}. Best is trial 1 with value: 0.9783889462472363.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 14:49:10,003]\u001b[0m Trial 5 finished with value: 0.9628847511096567 and parameters: {'n_estimator': 20, 'max_depth': 17}. Best is trial 1 with value: 0.9783889462472363.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 14:55:53,180]\u001b[0m Trial 6 finished with value: 0.9624095835207275 and parameters: {'n_estimator': 20, 'max_depth': 7}. Best is trial 1 with value: 0.9783889462472363.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 16:08:13,154]\u001b[0m Trial 7 finished with value: 0.9800381776037966 and parameters: {'n_estimator': 100, 'max_depth': 15}. Best is trial 7 with value: 0.9800381776037966.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"xgboost\"):\n",
    "    params = {}\n",
    "    params['MODEL_NAME'] = 'xgboost'\n",
    "    params['TRAIN_SIZE'] = len(X_train)\n",
    "    \n",
    "    start = time.time()\n",
    "    xgboost_params, xgboost_score = tune(xgboost_objective, study_name='xgboost')\n",
    "    params['time'] = time.time() - start\n",
    "    \n",
    "    params['n_estimators'] = xgboost_params['n_estimators']\n",
    "    params['max_depth'] = xgboost_params['max_depth']\n",
    "    #params['learning_rate'] = xgboost_params['learning_rate']\n",
    "    train_roc_auc = xgboost_score\n",
    "    mlflow.log_metric('train_roc_auc', train_roc_auc)\n",
    "    \n",
    "    # On entraine le modèle avec les paramètres retournés\n",
    "    clf_boost = GradientBoostingClassifier(n_estimators=params['n_estimators'],\n",
    "                                           max_depth=params['max_depth'],\n",
    "                                           #learning_rate=params['learning_rate'],\n",
    "                                           random_state=random_seed)\n",
    "    clf_boost.fit(X_train, y_train)\n",
    "    \n",
    "    # on calcule le score final\n",
    "    y_pred = clf_boost.predict(X_test)\n",
    "    mlflow.log_metric('test_roc_auc', roc_auc_score(y_test, y_pred))\n",
    "     \n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # On sauvegarde le modèle\n",
    "    mlflow.sklearn.log_model(clf_boost, 'XGBOOST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c1c35-c3b5-4163-9d54-4f512623c5f1",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcfe4b4d-a7b9-437f-805e-89e1df02fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def lightgbm_objective(trial):\n",
    "    _num_leaves = trial.suggest_int(\"num_leaves\", 50, 100)\n",
    "    _max_depth = trial.suggest_int(\"max_depth\", 1, 20)\n",
    "    _learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 1)\n",
    "    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 2000)\n",
    "    _min_child_weight = trial.suggest_float(\"min_child_weight\", 0.1, 10)\n",
    "    _reg_alpha = trial.suggest_float('reg_alpha', 0.01, 10)\n",
    "    _reg_lambda = trial.suggest_float('reg_lambda', 0.01, 10)\n",
    "    _subsample = trial.suggest_float('subsample', 0.01, 1)\n",
    "    \n",
    "    \n",
    "    clf_lightgbm = LGBMClassifier(\n",
    "                                  num_leaves=_num_leaves,\n",
    "                                  max_depth=_max_depth,\n",
    "                                  learning_rate=_learning_rate,\n",
    "                                  n_estimators=_n_estimators,\n",
    "                                  min_child_weight=_min_child_weight,\n",
    "                                  subsample=_subsample,\n",
    "                                  reg_alpha=_reg_alpha,\n",
    "                                  reg_lambda=_reg_lambda,\n",
    "                                  random_state=random_seed,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    score = cross_val_score(clf_lightgbm, X_train, y_train, cv=kfolds, scoring='roc_auc').mean()\n",
    "    \n",
    "    return score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11910ee3-76db-4997-8bd4-8638e8a9c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-04 16:56:17,151]\u001b[0m A new study created in memory with name: lightgbm\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 16:58:20,929]\u001b[0m Trial 0 finished with value: 0.9772113205102875 and parameters: {'num_leaves': 65, 'max_depth': 5, 'learning_rate': 0.070429067290123, 'n_estimators': 1948, 'min_child_weight': 5.109067046621498, 'reg_alpha': 6.372934911029781, 'reg_lambda': 7.917585220303045, 'subsample': 0.2719627749838453}. Best is trial 0 with value: 0.9772113205102875.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 16:58:33,702]\u001b[0m Trial 1 finished with value: 0.9770049119107733 and parameters: {'num_leaves': 86, 'max_depth': 3, 'learning_rate': 0.2840977927132267, 'n_estimators': 262, 'min_child_weight': 1.1063609272991082, 'reg_alpha': 2.2975188923242045, 'reg_lambda': 5.556815899726475, 'subsample': 0.031126658685631817}. Best is trial 0 with value: 0.9772113205102875.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 16:59:01,477]\u001b[0m Trial 2 finished with value: 0.975200691063123 and parameters: {'num_leaves': 66, 'max_depth': 14, 'learning_rate': 0.5419495407710921, 'n_estimators': 570, 'min_child_weight': 7.848563693869069, 'reg_alpha': 5.925841356025272, 'reg_lambda': 7.9928609802624075, 'subsample': 0.17801627044577986}. Best is trial 0 with value: 0.9772113205102875.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 16:59:48,920]\u001b[0m Trial 3 finished with value: 0.9776374484639465 and parameters: {'num_leaves': 83, 'max_depth': 16, 'learning_rate': 0.1676109150247204, 'n_estimators': 776, 'min_child_weight': 6.11279706949136, 'reg_alpha': 8.045933627364741, 'reg_lambda': 8.010585967743712, 'subsample': 0.17971387709779318}. Best is trial 3 with value: 0.9776374484639465.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 17:00:24,390]\u001b[0m Trial 4 finished with value: 0.9762782366326143 and parameters: {'num_leaves': 84, 'max_depth': 9, 'learning_rate': 0.353065056946718, 'n_estimators': 351, 'min_child_weight': 6.119212563622858, 'reg_alpha': 2.960207628901846, 'reg_lambda': 6.716269658456772, 'subsample': 0.1298672753032875}. Best is trial 3 with value: 0.9776374484639465.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 17:01:18,281]\u001b[0m Trial 5 finished with value: 0.9764333897197283 and parameters: {'num_leaves': 94, 'max_depth': 14, 'learning_rate': 0.3724912067957032, 'n_estimators': 1592, 'min_child_weight': 8.149368822368757, 'reg_alpha': 5.325529522720447, 'reg_lambda': 7.450705370960876, 'subsample': 0.43394515989263516}. Best is trial 3 with value: 0.9776374484639465.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 17:02:22,673]\u001b[0m Trial 6 finished with value: 0.9773432271853875 and parameters: {'num_leaves': 60, 'max_depth': 9, 'learning_rate': 0.14843086071851802, 'n_estimators': 586, 'min_child_weight': 1.0416377333537534, 'reg_alpha': 4.847629322132786, 'reg_lambda': 8.359291269188159, 'subsample': 0.352571476526596}. Best is trial 3 with value: 0.9776374484639465.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 17:03:14,701]\u001b[0m Trial 7 finished with value: 0.9737926515952463 and parameters: {'num_leaves': 59, 'max_depth': 14, 'learning_rate': 0.7418038837979201, 'n_estimators': 1635, 'min_child_weight': 7.085163569702138, 'reg_alpha': 2.5587333513512416, 'reg_lambda': 2.0199713588160506, 'subsample': 0.5996353423874823}. Best is trial 3 with value: 0.9776374484639465.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 17:03:20,854]\u001b[0m Trial 8 finished with value: 0.9744581964214956 and parameters: {'num_leaves': 68, 'max_depth': 4, 'learning_rate': 0.786202462660456, 'n_estimators': 63, 'min_child_weight': 6.814776131252214, 'reg_alpha': 7.234959841299178, 'reg_lambda': 3.901151680977903, 'subsample': 0.7942392501674669}. Best is trial 3 with value: 0.9776374484639465.\u001b[0m\n",
      "\u001b[32m[I 2023-01-04 17:03:37,428]\u001b[0m Trial 9 finished with value: 0.9768786565147802 and parameters: {'num_leaves': 56, 'max_depth': 17, 'learning_rate': 0.06052134308847612, 'n_estimators': 100, 'min_child_weight': 4.621562715437391, 'reg_alpha': 7.027725195588294, 'reg_lambda': 7.803923405901197, 'subsample': 0.1599770101987713}. Best is trial 3 with value: 0.9776374484639465.\u001b[0m\n",
      "C:\\Users\\matth\\miniconda3\\lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='light gbm'):\n",
    "    params = {}\n",
    "    params['MODEL_NAME'] = 'light gbm'\n",
    "    params['TRAIN_SIZE'] = len(X_train)\n",
    "    \n",
    "    start = time.time()\n",
    "    lgbmt_params, lgbm_score = tune(lightgbm_objective, study_name='lightgbm')\n",
    "    params['time'] = time.time() - start\n",
    "    \n",
    "    params['n_estimators'] = lgbmt_params['n_estimators']\n",
    "    params['max_depth'] = lgbmt_params['max_depth']\n",
    "    params['learning_rate'] = lgbmt_params['learning_rate']\n",
    "    params['num leaves'] = lgbmt_params['num_leaves']\n",
    "    params['min child weight'] = lgbmt_params['min_child_weight']\n",
    "    params['reg alpha'] = lgbmt_params['reg_alpha']\n",
    "    params['reg lambda'] = lgbmt_params['reg_lambda']\n",
    "    params['subsample'] = lgbmt_params['subsample']\n",
    "    mlflow.log_metric('train_roc_auc', lgbm_score)\n",
    "    \n",
    "    # On entraine le modèle avec les paramètres retournés\n",
    "    clf_lgbm = LGBMClassifier(n_estimators=params['n_estimators'],\n",
    "                              max_depth=params['max_depth'],\n",
    "                              learning_rate=params['learning_rate'],\n",
    "                              num_leaves=params['num leaves'],\n",
    "                              min_child_weight=params['min child weight'],\n",
    "                              reg_alpha=params['reg alpha'],\n",
    "                              reg_lambda=params['reg lambda'],\n",
    "                              subsample=params['subsample'],\n",
    "                              random_state=random_seed)\n",
    "    clf_lgbm.fit(X_train, y_train)\n",
    "    \n",
    "    # On calcule le score final\n",
    "    y_pred = clf_lgbm.predict(X_test)\n",
    "    mlflow.log_metric('test_roc_auc', roc_auc_score(y_test, y_pred))\n",
    "    \n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # On sauvegarde le modèle\n",
    "    mlflow.sklearn.log_model(clf_lgbm, 'LGBM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f6c37-6b53-4fb0-b790-3264df55e2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
